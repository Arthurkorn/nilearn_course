{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this section, we will learn to extract signals in ROIs from fMRI data and compute a connectome with them."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Extracting signal on a parcellation"
     ]
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "We are going to load the Harvard-Oxford atlas, in it's 'max-prob' version, ie the non probabilistic version:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nilearn import datasets, plotting\n",
      "atlas_img, labels = datasets.load_harvard_oxford('cort-maxprob-thr25-2mm')\n",
      "plotting.plot_roi(atlas_img)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To extract signals on this atlas, we create an \"Masker\" object, that can transform niftis in time-series using a set of labels (the non-probabilistic atlas)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nilearn.input_data import NiftiLabelsMasker\n",
      "masker = NiftiLabelsMasker(labels_img=atlas_img)\n",
      "\n",
      "# Now get the ADHD data\n",
      "data = datasets.fetch_adhd(n_subjects=1)\n",
      "\n",
      "# And apply the masker to the functional data\n",
      "time_series = masker.fit_transform(data.func[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This take a little time. To avoid recomputing steps later, we can use the 'memory' argument of the masker. Also, to follow what's happening, we can use the 'verbose' argument.\n",
      "\n",
      "Now our 'time_series' should be a numpy array of shape: (time, n_labels)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "time_series.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can plot these time-series using matplotlib:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from matplotlib import pyplot as plt\n",
      "plot = plt.plot(time_series)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can immediatly see that there an ROI-specific offset that is of little interest to us. Let's standardize time-series during extraction (center and norm them), with the \"standardize\" option of the masker:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "masker = NiftiLabelsMasker(labels_img=atlas_img, standardize=True,\n",
      "                           memory='/tmp/nilearn_course', verbose=5)\n",
      "\n",
      "time_series = masker.fit_transform(data.func[0])\n",
      "plot = plt.plot(time_series)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can now use numpy to plot a correlation matrix, which will be the correlation of the signals across ROIs"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "correlation = np.corrcoef(time_series.T)\n",
      "plt.figure(figsize=(10, 10))\n",
      "plt.imshow(correlation, interpolation=\"nearest\")\n",
      "x_ticks = plt.xticks(range(len(labels) - 1), labels[1:], rotation=90)\n",
      "y_ticks = plt.yticks(range(len(labels) - 1), labels[1:])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is somewhat messy isn't it: everything is connected to everything.\n",
      "\n",
      "What did we forget?\n",
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Improving the connectivity matrix"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Exercise**: Improve this connectivity matrix\n",
      "\n",
      "**Hint**: What in 'data.keys' can be used to improve the connectivity matrix? Look at the arguments of masker.fit_transform to find how to use it."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load adhd_connectivity.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Better? Maybe, but one problem is that the atlas does not resolve functional structures: if we look at which regions the PCC correlates strongly with, we are expecting to find the various parts of the default mode network. However, there are no parietal regions, as the parietal lobe is not divided properly in the Harvard Oxford atlas.\n",
      "\n",
      "Thus, it is of interest to use an atlas of function regions."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Extracting signal on a probabilistic atlas"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can use the MSDL atlas (Varoquaux, IPMI 2011), which is a probabilistic atlas: it is a set of weighted maps that capture functional regions at rest."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "atlas = datasets.fetch_msdl_atlas()\n",
      "atlas_img = atlas['maps']\n",
      "labels = atlas['labels']\n",
      "\n",
      "# Plot the regions defining the default-mode network\n",
      "import nibabel\n",
      "atlas_img = nibabel.load(atlas_img)\n",
      "display = plotting.plot_stat_map(nibabel.Nifti1Image(atlas_img.get_data()[..., 4],\n",
      "                                                     atlas_img.get_affine()),\n",
      "                        colorbar=False)\n",
      "display.add_overlay(nibabel.Nifti1Image(atlas_img.get_data()[..., 5],\n",
      "                                                     atlas_img.get_affine()),\n",
      "                    cmap=plotting.cm.black_blue, vmax=.5, vmin=0)\n",
      "display.add_overlay(nibabel.Nifti1Image(atlas_img.get_data()[..., 6],\n",
      "                                                     atlas_img.get_affine()),\n",
      "                    cmap=plotting.cm.black_green, vmax=.5, vmin=0)\n",
      "display.add_overlay(nibabel.Nifti1Image(atlas_img.get_data()[..., 3],\n",
      "                                                     atlas_img.get_affine()),\n",
      "                    cmap=plotting.cm.black_pink, vmax=.5, vmin=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To extract time-series on such regions, \"softly\" defined by continuous maps, we should use nilearn.input_data.NiftiMapsMasker.\n",
      "\n",
      "**Excercise**: By analogy with the above, compute and display a correlation matrix on the MSDL atlas.\n",
      "\n",
      "**Note**: On nilearn 1.0 beta, it is not possible to retrieve correct labels for the MSDL atlas. This has been fixed in later versions."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Solution\n",
      "%load adhd_msdl_connectivity.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Excercise** Compute the corresponding correlation matrix for a few other subjects in the ADHD dataset."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Inverse covariance estimation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can start by using the covariance estimators in scikit-learn"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import covariance\n",
      "estimator = covariance.EmpiricalCovariance(store_precision=True)\n",
      "estimator.fit(time_series)\n",
      "plt.matshow(estimator.precision_)\n",
      "plt.title('Non regularized inverse covariance')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Excercise**: Now use covariance.GraphLassoCV to estimate a sparse inverse covariance."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, give that we have multiple subjects, we can use the group-sparse model, to have better sparse recovery. For this we use the corresponding model in nilearn.covariance. It requires several time-series."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Get the ADHD data for multiple subjects\n",
      "data = datasets.fetch_adhd(n_subjects=4)\n",
      "\n",
      "# And apply the masker to the functional data of each subject\n",
      "time_series = list()\n",
      "for subject_data, subject_confounds in zip(data.func, data.confounds):\n",
      "     time_series.append(masker.fit_transform(subject_data, confounds=subject_confounds))\n",
      "\n",
      "from nilearn.group_sparse_covariance import GroupSparseCovarianceCV\n",
      "estimator = GroupSparseCovarianceCV()\n",
      "estimator.fit(time_series)\n",
      "for precision in estimator.precisions_.T:\n",
      "    plt.matshow(precision)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}